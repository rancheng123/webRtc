<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Document</title>
    <script src="./utils/index.js"></script>
</head>
<body>








</body>
<style>
    .me{
        border: 2px solid greenyellow;
    }
    .other{
        border: 2px solid blue;
    }

    .meeting{
        width: 200px;
        height: 200px;
        float: left;
    }
    .screenShare{
        width: 100px;
        height: 100px;
        float: left;
    }
    .screenShare video,.meeting video{
        width: 100%;
    }

</style>
<script>






    /*
        参考文档
        https://www.html5rocks.com/en/tutorials/webrtc/basics/#toc-sans

        //domes
        https://webrtc.github.io/samples/

    */


    /*
        常见问题：
            投屏卡顿：  内存泄漏，Cpu 飙升

            投屏掉线：  网络环境差


    */


    /*
        调试 和分析 webRtc
            工具：
                https://test.webrtc.org/

                chrome://webrtc-internals
                chrome://flags/

                    1.RTCPeerConnection事件  失败将显示红色。
                    2.统计数据列表

                        ssrc（Stream Source）表示流源

                            绍了轨道的吞吐量，RTCP往返时间

                        bweforvideo   对等连接的带宽估计报告



                        //获取统计数据
                        pc.getStats(function(stats) {
                           debugger
                        });

    */


    /*
    自己没有实现的功能：
        1. 有人说话时，micphone 图标 变亮
            zoom 的实现方法:   socket服务端 返回音量，前端展示
            {
               "body" : {
                  //用户1 的音量
                  "asn1" : 16778240,
                  "asn2" : 16779264
               },
               "evt" : 12033,
               "seq" : 157
            }
}

    */



    //直播中，H5实时解码音频并播放
    //https://www.jianshu.com/p/d45031b5d2c1









    var userId = parseUrl(location.href).params.userId;

    var otherUserId = null;
    var pc;
    var pcMaps = {};
    var ws;
    var connections;


    //本地视频流（当前用户自己的视频流）
    var localStream = null;
    //meeting 视频会议     screenShare 投屏
    var type = parseUrl(location.href).params.type;

    start()
    function start(){

        if(!userId){
            console.error('缺少参数 userId, 请输入正确的url,如屏幕类型         https://10.28.12.83:9001/meeting.html?userId=screen \\\n'+
                '                                用户类型 视频会议 https://10.28.12.83:9001/meeting.html?userId=1&type=meeting  \\\n' +
                '                                用户类型 投屏     https://10.28.12.83:9001/meeting.html?userId=2&type=screenShare  \\\n'

            );
            return;
        }

        //
        if(userId != 'screen'){
            if(!type){
                console.error('缺少参数 type, 请输入正确的url,如 https://10.28.12.83:9001/meeting.html?type=meeting&userId=2');
                return
            }
        }








        //信令
        ws = new WebSocket('wss://' + location.host);


        ws.wsSend = function(msgObj){
            if(ws.timer){
                clearInterval(ws.timer)
            }



            this.send(JSON.stringify({
                'from': userId,
                ...msgObj
            }));

            //长时间无反应后， 发送检查包
            ws.timer = setInterval(()=>{
                this.send(JSON.stringify({
                    'data': '3'
                }));
            },30000)
        }

        ws.onopen = (event)=>{
            //加入会议
            ws.wsSend({
                'action' : 'join'
            });
        }


        //此处是 调试webRtc 的一个重要区域
        ws.onmessage = function (event) {
            var data = JSON.parse(event.data);

            switch ( data.action) {
                case 'answer' :


                    pc.setRemoteDescription(new RTCSessionDescription(data.answer))
                    break
                case "candidate":


                    //接受远端传来的candidate
                    pc.addIceCandidate(new RTCIceCandidate(data.candidate))


                    /*
                    data.candidate.candidate 里面的type 参数

                        1.host                                   表示   与本地局域网上设备  的连接
                        2.srflx（服务器反身性Server Reflexive）    表示   通过公网Ip转换      的连接
                        3.relay                                  用于描述TURN的连接性。当双方都提供这样的数据包时，那么连接是绝对可能进行的。

                    */


                    break
                case "create":

                    otherUserId = data.from

                    reactiveSendLocalStream(data.offer,data.from)
                    break

                //由用户关闭
                case "close" :
                    deleteStream({
                        userId: data.closedUserId
                    })

                    break


                case "joinSuccess":

                    // 屏幕   加入房间后，展示房间号
                    if(userId == 'screen'){



                    }
                    // 只有客户（非屏幕）才开启摄像头
                    else{

                        ws.wsSend({
                            'action' : 'getConnections'
                        });



                    }



                    break

                case "getConnectionsSuccess":

                    connections = data.data
                    openStream()
                    break
            }
        }








    }












    function openStream(){
        //视频会议
        if(type == 'meeting'){
            getStream()
        }
        //投屏
        else{
            winRequest('getScreenCaptureSourceId',(event)=>{
                console.log(event.data.data)

                getStream(event.data.data)
            })
        }
    }

    function deleteStream(opts){
        var videoWrap = document.getElementById(opts.userId);
        if(videoWrap){

            var stream = videoWrap.querySelector('video').srcObject
            if(1){
                //第一种方法
                stream.getVideoTracks().forEach((videoTrack)=>{
                    videoTrack.stop()
                })
            }else{
                //第二种方法
                var sender = pc.getSenders()[0]
                pc.removeTrack(sender);
            }
            document.body.removeChild(videoWrap)

            //关闭 点对点链接  节约性能
            pcMaps[opts.userId].close()
            delete pcMaps[opts.userId]
        }




    }




    function getStream(sourceId){


        if(type == 'meeting'){
            var config = {
                audio: true,
                /*
                audio: {
                    // 音频的采样率
                    sampleRate: '',
                    //  音频的采样大小
                    sampleSize: '',
                    // 音量
                    volume: '',
                    // 噪声抑制
                    noiseSuppression: true,
                    // 回声消除(不兼容，有点浏览器不支持)
                    echoCancellation: false
                },
                */
                video: {
                    mandatory: {},
                    optional: [],

                    width: {
                        exact: 1280
                    }
                    //获取一个最接近 1280x720 的相机分辨率
                    // width: 1280, height: 720,
                    //
                    // //帧率
                    // frameRate: {
                    //     ideal: 10,
                    //     max: 15
                    // },
                    //
                    // //前置或者后置摄像头  "user" : "environment"
                    // facingMode: "user"
                }
            }
        }else{
            var config = {
                // 获取桌面屏幕  audio 必须是false, 否则报错
                audio: false,
                video: {
                    mandatory: {
                        chromeMediaSource: 'desktop',
                        chromeMediaSourceId: sourceId,
                        minWidth: 1280,
                        maxWidth: 1920,
                        minHeight: 720,
                        maxHeight: 1080
                    }
                }
            }
        }

        navigator.webkitGetUserMedia(config, (stream)=>{
            localStream = stream;


            showStream(localStream,userId)



            //除了自己，向其他人和屏幕依次发起视频会议
            var otherUsers = connections.users.filter((ele,i)=>{
                return ele != userId
            })

            launchVideoByQueue(otherUsers);







            return;





            //判断是否支持这种类型
            MediaRecorder.isTypeSupported('video/webm;codecs=vp9')

            //音视频采集
            var mediaRecorder = new MediaRecorder(stream, {
                audioBitsPerSecond : 128000, // 音频码率
                videoBitsPerSecond : 100000, // 视频码率

                // 编码格式
                mimeType : 'video/webm' +
                // 编码格式
                ';codecs=h264'

                //codecs=h264
                //codecs=vp8
                //codecs=vp9


            })
            mediaRecorder.start()


            //停止录制. 同时触发dataavailable事件
            //mediaRecorder.stop()

            mediaRecorder.ondataavailable = function (e) {
                console.log(e)
                // 下载视频
                var blob = new Blob([e.data], { 'type' : 'video/mp4' })
                let a = document.createElement('a')
                a.href = URL.createObjectURL(blob)
                a.download = `test.mp4`
                a.click()
            }


        }, (e)=>{
            debugger
        })





    }


    function controllStream(){
        return;




        remoteStream.onactive = ()=>{
            debugger
        }
        remoteStream.oninactive = ()=>{
            debugger
        }
        remoteStream.onaddtrack = ()=>{
            debugger
        }
        remoteStream.onremovetrack = ()=>{
            debugger
        }
        remoteStream.ondatachannel = ()=>{
            debugger
        }


        window.remoteStream = remoteStream;
        window.videoTrack = remoteStream.getVideoTracks()[0];


        //关闭视频
        //videoTrack.stop()

        localStream.getAudioTracks().forEach(function(e) {
            e.enabled = false
        })

        remoteStream.getAudioTracks().forEach(function(e) {
            e.enabled = false
        })

        videoTrack.onended = ()=>{
            debugger
        }
        videoTrack.onunmute = ()=>{
            debugger
        }
        videoTrack.onmute = ()=>{
            debugger
        }


        if(1){

            var constraints = {
                video: true,
                audio: {
                    "sampleSize": 16,
                    "channelCount": 2,
                    "echoCancellation": false
                }
            }


            //测试 start
            navigator.mediaDevices.getUserMedia(constraints).then(function(stream) {

                localStream = stream;
                var localAudioTrack = localStream.getAudioTracks()[0];

                localStream.getAudioTracks()[0].enabled = false

                //window.cloneTrack = localAudioTrack.clone();
                //localAudioTrack.stop();



                //window.newStream = new MediaStream()
                //newStream.addTrack(cloneTrack)







                let audioTracks = stream.getAudioTracks();
                let videoTracks = stream.getVideoTracks();


                if (audioTracks.length) {
                    audioTrack = audioTracks[0];
                    audioTrack.getSettings()


                    //动态更改限制
                    constraints.audio.echoCancellation = true;
                    audioTrack.applyConstraints({
                        "echoCancellation": true
                    }).then(() => {

                        /* do stuff if constraints applied successfully */
                    }).catch(function(reason) {
                        debugger
                        /* failed to apply constraints; reason is why */
                    });

                }
                if (videoTracks.length) {

                    videoTrack = videoTracks[0];

                    videoTrack.getSettings()

                    videoTrack.getConstraints();




                    videoTrack.applyConstraints({

                        //前置摄像头
                        //facingMode: 'user',

                        //后置摄像头
                        facingMode: { exact: "environment" }
                    }).then(() => {

                        /* do stuff if constraints applied successfully */
                    }).catch(function(reason) {

                        /* failed to apply constraints; reason is why */
                    });
                }
                return stream
            }).then(function(stream) {
                localStream = stream;

                showStream(localStream,userId)



                //除了自己，向其他人和屏幕依次发起视频会议
                var otherUsers = connections.users.filter((ele,i)=>{
                    return ele != userId
                })

                launchVideoByQueue(otherUsers);
            }).then(function() {
                getCurrentSettings();
            }).catch((err)=>{
                console.log(err)

            });

            //测试 end
        }else{

        }






        return;
        //设备相关
        navigator.mediaDevices.enumerateDevices()
            .then(devices => {

                console.log(devices)

                var micPhone = devices[1];
                var speaker = devices[3];
                var camera = devices[2]





                //测试 start
                config = {
                    audio: {
                        optional: [{
                            deviceId: speaker.deviceId
                        }]
                    },
                    video: {
                        optional: [{
                            sourceId: camera.deviceId
                        }]
                    }
                }
                //测试 end



            });


        //获取支持的能力
        navigator.mediaDevices.getSupportedConstraints()
    }



    //按循序发起视频会议
    function launchVideoByQueue(otherUsers){

        otherUserId = otherUsers.pop();
        if(otherUserId){
            initiativeSendLocalStream()



            //此处需要优化
            setTimeout(()=>{
                launchVideoByQueue(otherUsers)
            },1000)

        }
    }


    //被动发送本地流
    function reactiveSendLocalStream(offer){
        if(!pcMaps[otherUserId]){
            pc = pcMaps[otherUserId] = createPeerConnection();
        }else{
            pc = pcMaps[otherUserId]
        }


        pc.setRemoteDescription(new RTCSessionDescription(offer));

        //用户（非屏幕）返回视频流
        if(userId != 'screen'){
            pc.addStream(localStream)
        }

        pc.createAnswer(function (answer) {
            pc.setLocalDescription(answer);



            pc.users.me=userId;
            pc.users.he=otherUserId;
            ws.wsSend({
                'to': otherUserId,
                'action' : 'answer' ,
                'answer' : answer
            });
            //offerer.setRemoteDescription(answer);
        }, function() {}, {
            optional: [],
            mandatory: {
                OfferToReceiveAudio: true,
                OfferToReceiveVideo: true
            }
        });
    }




    function createPeerConnection(){
        /*
           webRtc(Web Real-Time Communications)
               实时通讯
               作用：建立点对点（Peer-to-Peer）的连接，实现视频流，音频流的传输
               应用场景：数据分享和电话会议
               支持： 浏览器，原生移动app(andiro,ios)
               核心技术:
                    音视频的采集、编解码、网络传输、显示

                    视频
                        采集
                            支持多种媒体类型，比如I420、YUY2、RGB、UYUY等，
                            可以帧大小和帧率控制。
                        编解码
                            I420

                            VP8
                                能以更少的数据提供更高质量的视频，特别适合视频会议这样的需求


                            Flv、Mp4、Mov可以理解为是容器，里面是编码的音视频数据


                        加密 (video_engine_encryption)
                            会有性能的代价
                        媒体文件
                            1.可以用本地文件作为视频源
                            2.可以录制音视频到本地文件

                        图像处理
                            针对每一帧的图像进行处理，包括明暗度检测、颜色增强、降噪处理等功能，用来提升视频质量。
                        显示

                        网络传输与流控
                            RTP
                            RTCP

                   音频
                        音频设备
                        编解码
                            iLIBC/iSAC/G722/PCM16/RED/AVT



               webRtc 架构

                   webApi(js api):
                        Network Stream API
                            MediaStream        表示一个媒体数据流
                            MediaStreamTrack   表示一个媒体源

                        RTCPeerConnection
                            RTCPeerConnection  允许在浏览器间通讯
                            RTCIceCandidate    表示一个ICE协议的候选者
                            RTCIceServer       表示一个ICE协议 Server。


                            ICE (Interactive Connectivity Establishment,交互式连接建立)
                                本质（网络地址转换协议）
                                    是一种NAT传输协议（建立在STUN协议和TURN协议之上）

                                    NAT（Network Address Translation，网络地址转换）
                                作用
                                    为打通不同局域网的链接， （数据包报头中的ip地址转换） 将本地IP地址  转成  公共IP地址
                                    客户端侧无需关心所处网络的位置以及NAT类型，并且能够动态的发现最优的传输路径。


                                        3中情况
                                            1.同一局域网内，两台机器本地IP地址之间    可连接

                                            2.不同局域网内，两台机器本地IP地址之间  不可连接
                                            3.不同局域网内，两台机器公共IP地址之间    可连接



                                工作流程
                                    1.收集本机的 candidates


                                        每个端 有多个传输地址候选（CANDIDATE）, 可以设置Candidate的优先级

                                            HOST             CANDIDATE
                                            SERVER REFLEXIVE CANDIDATES
                                            RELAYED          CANDIDATES

                                    2.交换candidates
                                        ICE 使用offer/answer方式，双方通过SDP协商交换candidate信息.

                                    3.生成多个候选对     （checklist  等待连通性检查验证的候选对） （候选地址有多个，但并不是都能用）

                                    4.生成多个可用的候选对（validlist  通过连通性检查验证的候选对）

                                    5.选择最终传输地址

                                        ICE选择高优先级的可以联通的candidate pair。








                            //代表了一端的会话能力
                            SDP（Session Description Protocol，会话描述协议）
                                本质
                                    是一种标准（描述信息）
                                组成
                                    会话名称
                                    会话目的
                                    会话的激活时间
                                    会话的媒体(media)
                                    会话的带宽信息
                                    会话拥有者的联系信息

                                Offer/Answer模型

                                    offer
                                        本质
                                            SDP报文
                                        包含
                                            多媒体流
                                            编解码方法

                                        角色
                                            controlling角色 （控制方）

                                                ICE角色冲突解决
                                                    当两端角色都为controlling或者controlled角色冲突时，在连通性检查阶段，要求发送binding request消息里必须要带上tie-breaker属性。
                                                    当出现冲突时，比较tie-breaker大小，值比较大的则被认为是controlling，同时回应487错误给对端，对端收到487错误后切换角色。

                                    answer
                                        本质
                                            SDP报文
                                        包含
                                            多媒体流
                                            编解码方法

                                        角色
                                            controlled角色 （被控制方）


                                 组成解释

                                 "
                                    //sdp版本号，一直为0,rfc4566规定
                                    v=0


                                    o=  -                    (username,没有使用-代替)
                                        397228656339456119   (sess-id)
                                        5                    (sess-version, 过程中有改变编码之类的操作，重新生成sdp时,sess-version加1)
                                        IN                   (nettype)
                                        IP4                  (addrtype)
                                        127.0.0.1            (unicast-address)


                                    s=-                      (会话名，没有的话使用-代替)


                                    t=0 0                    (两个值分别是会话的起始时间和结束时间，这里都是0代表没有限制)


                                    a=group:BUNDLE audio video data  (说明音频，视频，数据共用一个传输通道，如果没有这一行，音视频，数据就会分别单独用一个udp端口来发送)


                                    (定义Media Stream ID，   WMS是WebRTC Media Stream简称，   一般定义了这个，后面a=ssrc这一行就会有msid,mslabel等属性)
                                    a=msid-semantic: WMS h1aZ20mbQB0GSsq0YxLfJmiYWE9CBfGch97C





                                    //音频部分
                                        m=
                                            audio  (说明本会话包含音频)
                                            60551  (传输音频的端口)
                                            UDP/TLS/RTP/SAVPF  （来传输音频支持的协议）
                                            111 103 9 0 8 106 105 13 110 112 113 126 （表示本会话音频支持的编码）


                                        c=IN IP4 10.28.12.40  （IP地址，用来接收或发送音频，   webrtc使用ice传输，不使用这个地址）


                                        a=rtcp:9 IN IP4 0.0.0.0 （用来传输rtcp地地址和端口，webrtc中不使用）


                                        a=candidate:704553097 1 udp 2122260223 10.28.12.40 60551 typ host generation 0 network-id 4 network-cost 10
                                        a=candidate:559267639 1 udp 2122136831 ::1 36717 typ host generation 0 network-id 2
                                        a=candidate:1510613869 1 udp 2122063615 127.0.0.1 52474 typ host generation 0 network-id 1


                                        a=ice-ufrag:hhy2                    （ice协商过程中的安全验证信息）
                                        a=ice-pwd:WAFAUxaAjPJeOlYyZ7p9trKf  （ice协商过程中的安全验证信息）
                                        a=ice-options:renomination


                                       （dtls协商过程中需要的认证信息）
                                        a=fingerprint:sha-256 2F:7C:77:DD:F9:D1:53:BB:0E:66:CD:9C:F4:0E:AD:82:04:0E:00:84:E5:13:6F:8D:AA:79:C8:99:D0:E7:E2:6B


                                        a=setup:active  （代表本客户端在dtls协商过程中，可以做客户端也可以做服务端，参考rfc4145 rfc4572）


                                        a=mid:audio  （媒体标识）


                                        a=extmap:1 urn:ietf:params:rtp-hdrext:ssrc-audio-level  （在rtp头部中加入音量信息，参考 rfc6464）


                                        a=inactive  （通信方向，
                                                            recvonly  只接收,
                                                            sendonly  只发送,
                                                            sendrecv  接受  也发送,
                                                            inactive  不接受 不发送
                                                     ）



                                        a=rtcp-mux  （指出rtp,rtcp包使用同一个端口来传输）



                                        a=rtcp-fb:111 transport-cc  （说明opus编码支持使用rtcp来控制拥塞，参考https://tools.ietf.org/html/draft-holmer-rmcat-transport-wide-cc-extensions-01）



                                        a=fmtp:111
                                            minptime=10; （代表最小打包时长是10ms）
                                            useinbandfec=1 （代表使用opus编码内置fec特性）


                                        （下面几行都是对m=audio这一行的媒体编码补充说明，指出了编码采用的编号，采样率，声道等 ）
                                        a=rtpmap:111 opus/48000/2
                                        a=rtpmap:103 ISAC/16000
                                        a=rtpmap:9 G722/8000
                                        a=rtpmap:0 PCMU/8000
                                        a=rtpmap:8 PCMA/8000
                                        a=rtpmap:106 CN/32000
                                        a=rtpmap:105 CN/16000
                                        a=rtpmap:13 CN/8000
                                        a=rtpmap:110 telephone-event/48000
                                        a=rtpmap:112 telephone-event/32000
                                        a=rtpmap:113 telephone-event/16000
                                        a=rtpmap:126 telephone-event/8000


                                        （用于音视频同步）
                                        a=
                                            ssrc:18509423                             （发生冲突时   可能会  发生变化）
                                            cname:sTjtznXLCNH7nbRw  （用来标识一个数据源， 发生冲突时   不     发生变化）
                                            //个人觉得cname  是 channelName


                                        （定义了ssrc和WebRTC中的MediaStream,AudioTrack之间的关系）
                                        a=
                                            ssrc:18509423
                                            msid:
                                                h1aZ20mbQB0GSsq0YxLfJmiYWE9CBfGch97C  （stream-d）
                                                15598a91-caf9-4fff-a28f-3082310b2b7a  （track-id）


                                        a=
                                            ssrc:18509423
                                            mslabel:h1aZ20mbQB0GSsq0YxLfJmiYWE9CBfGch97C  （stream-d）

                                        a=
                                            ssrc:18509423
                                            label:15598a91-caf9-4fff-a28f-3082310b2b7a    （track-id）




                                    //视频部分
                                        m=
                                            video  (说明本会话包含视频)
                                            9      (传输视频的端口)
                                            UDP/TLS/RTP/SAVPF   （来传输视频支持的协议）
                                            96 98 100 102 127 97 99 101 125   （表示本会话视频支持的编码）


                                        c=IN IP4 0.0.0.0  （IP地址，用来接收或发送视频，   webrtc使用ice传输，不使用这个地址）

                                        a=rtcp:9 IN IP4 0.0.0.0 （用来传输rtcp地地址和端口，webrtc中不使用）


                                        a=ice-ufrag:hhy2                    （ice协商过程中的安全验证信息）
                                        a=ice-pwd:WAFAUxaAjPJeOlYyZ7p9trKf  （ice协商过程中的安全验证信息）
                                        a=ice-options:renomination



                                        （dtls协商过程中需要的认证信息）
                                        a=fingerprint:sha-256 2F:7C:77:DD:F9:D1:53:BB:0E:66:CD:9C:F4:0E:AD:82:04:0E:00:84:E5:13:6F:8D:AA:79:C8:99:D0:E7:E2:6B

                                        a=setup:active    （代表本客户端在dtls协商过程中，可以做客户端也可以做服务端，参考rfc4145 rfc4572）

                                        a=mid:video       （媒体标识）



                                        a=extmap:2 urn:ietf:params:rtp-hdrext:toffset
                                        a=extmap:3 http://www.webrtc.org/experiments/rtp-hdrext/abs-send-time
                                        a=extmap:4 urn:3gpp:video-orientation
                                        a=extmap:5 http://www.ietf.org/id/draft-holmer-rmcat-transport-wide-cc-extensions-01
                                        a=extmap:6 http://www.webrtc.org/experiments/rtp-hdrext/playout-delay




                                        a=recvonly  （通信类型，
                                                                recvonly只接收,
                                                                sendonly只发送,
                                                                sendrecv接受也发送,
                                                                inactive 正在激活
                                                         ）



                                        a=rtcp-mux  （指出rtp,rtcp包使用同一个端口来传输）


                                        a=rtcp-rsize






                                        a=rtpmap:96 VP8/90000
                                        (简称，意思是接收方通知发送方发送幅完全帧过来)
                                        a=rtcp-fb:96
                                            ccm (ccm是codec control using RTCP feedback message简称，意思是支持使用rtcp反馈机制来实现编码控制)
                                            fir (fir是Full Intra Request)

                                        a=rtcp-fb:96 nack  (支持丢包重传，参考rfc4585)

                                        a=rtcp-fb:96 nack pli  (支持关键帧丢包重传,参考rfc4585)

                                        a=rtcp-fb:96 goog-remb  (支持使用rtcp包来控制发送方的码流)

                                        a=rtcp-fb:96 transport-cc  （说明opus编码支持使用rtcp来控制拥塞，参考https://tools.ietf.org/html/draft-holmer-rmcat-transport-wide-cc-extensions-01）





                                        a=rtpmap:98 VP9/90000
                                        a=rtcp-fb:98 ccm fir
                                        a=rtcp-fb:98 nack
                                        a=rtcp-fb:98 nack pli
                                        a=rtcp-fb:98 goog-remb
                                        a=rtcp-fb:98 transport-cc


                                        a=rtpmap:100 H264/90000
                                        a=rtcp-fb:100 ccm fir
                                        a=rtcp-fb:100 nack
                                        a=rtcp-fb:100 nack pli
                                        a=rtcp-fb:100 goog-remb
                                        a=rtcp-fb:100 transport-cc
                                        a=fmtp:100 level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=42e01f (h264编码可选的附加说明)



                                        a=rtpmap:102 red/90000      (fec冗余编码，一般如果sdp中有这一行的话，rtp头部负载类型就是116，否则就是各编码原生负责类型)

                                        a=rtpmap:127 ulpfec/90000   (支持ULP FEC，参考rfc5109)


                                        a=rtpmap:97 rtx/90000
                                        a=fmtp:97 apt=96      (以上两行是VP8编码的重传包rtp类型)



                                        a=rtpmap:99 rtx/90000
                                        a=fmtp:99 apt=98


                                        a=rtpmap:101 rtx/90000
                                        a=fmtp:101 apt=100



                                        a=rtpmap:125 rtx/90000
                                        a=fmtp:125 apt=102





                                        （用于音视频同步）
                                        a=
                                            ssrc:18509423                             （发生冲突时   可能会  发生变化）
                                            cname:sTjtznXLCNH7nbRw  （用来标识一个数据源， 发生冲突时   不     发生变化）
                                            //个人觉得cname  是 channelName


                                        （定义了ssrc和WebRTC中的MediaStream,VideoTrack之间的关系）
                                        a=
                                            ssrc:18509423
                                            msid:
                                                h1aZ20mbQB0GSsq0YxLfJmiYWE9CBfGch97C  （stream-d）
                                                15598a91-caf9-4fff-a28f-3082310b2b7a  （track-id）

                                        a=
                                            ssrc:18509423
                                            mslabel:h1aZ20mbQB0GSsq0YxLfJmiYWE9CBfGch97C  （stream-d）

                                        a=
                                            ssrc:18509423
                                            label:15598a91-caf9-4fff-a28f-3082310b2b7a    （track-id）





                                    m=application 9 DTLS/SCTP 5000
                                    c=IN IP4 0.0.0.0
                                    b=AS:30
                                    a=ice-ufrag:hhy2
                                    a=ice-pwd:WAFAUxaAjPJeOlYyZ7p9trKf
                                    a=ice-options:renomination
                                    a=fingerprint:sha-256 2F:7C:77:DD:F9:D1:53:BB:0E:66:CD:9C:F4:0E:AD:82:04:0E:00:84:E5:13:6F:8D:AA:79:C8:99:D0:E7:E2:6B
                                    a=setup:active
                                    a=mid:data
                                    a=sctpmap:5000 webrtc-datachannel 1024
                                    "





                        Peer-to-peer Data API

                            DataChannel        表示一个在两个节点之间的双向的数据通道 。


                   WebRTC Native C++ API
                        浏览器厂商基于C++Api 实现Web API

                   Transport / Session

                        a. RTP Stack协议栈
                            Real Time Protocol

                        b. STUN/ICE
                            通过STUN和ICE组件 建立不同类型网络间的呼叫连接。

                        c. Session Management
                            一个抽象的会话层，提供会话建立和管理功能。该层协议留给应用开发者自定义实现。

                    VoiceEngine
                        定义：
                            是一个框架（处理音频多媒体）

                        组成：
                            1.iSAC （宽带和超宽带音频编解码器）（Internet Speech Audio Codec）

                                采样频率：16khz，24khz，32khz；（默认为16khz）
                                自适应速率为10kbit/s ~ 52kbit/s；
                                自适应包大小：30~60ms；
                                算法延时：frame + 3ms
                            2.iLBC （窄带语音编解码器）（Internet Low Bitrate Codec）
                            3.NetEQ for Voice
                                自适应抖动控制算法
                                语音包丢失隐藏算法
                            4.AEC (Acoustic Echo Canceler)
                                回声消除器是一个处理信号元件（能实时的去除mic采集到的回声）。
                            5.NR (Noise Reduction)
                                噪声抑制也是一个信号处理元件（消除背景噪声，嘶嘶声，风扇噪音等等… …）


                    VideoEngine
                        定义：
                            是一个框架（处理视频多媒体）
                        组成：
                            1.VP8 （视频图像编解码器）

                            2.Video Jitter Buffer
                                视频抖动缓冲器（降低视频抖动和视频包丢失 影响）
                            3.Image enhancements
                                图像质量增强模块，提升视频质量（明暗度检测、颜色增强、降噪处理）

                                //李海 告诉我的知识

                                    丢包处理
                                        比如： 网络较差， 但是画面依然流畅。
                                                实现原理：
                                                    自动补充丢失桢画面
                                                        根据前后两个桢画面的动画趋势  计算并生成 丢失的桢画面（）

                                    性能优化

                                        只渲染动态画面，不渲染静态画面
                                            举例： 只渲染摇晃的人脸， 不渲染人身后的背景。



                影响视频流的因素
                    1. 网络抖动
                    2. 丢包
                    3. 采样频率




               问题： 会产生噪音（啸叫）
                    产生原因： 音响放出的声音  又传给了话筒 （KTV 经常产生啸叫）。
                    解决办法： 话筒和音响 不要离得太近。




              硬件要求
                    设备要求： zoom 在windows 上运行， 不推荐使用andrio  ,对于硬件要求高
                    网络要求： 10方/2M
                    编码： 视频流压缩
                    解码： 视频流解压缩


                    摄像头分辨率：720P,1080P, 2K,4K


                    丢包



       */


        //Adapter.js 解决webRtc 的兼容性，

        //  document.createElement\("audio

        //一个WebRTC连接(本地到远端)


        var pc = new webkitRTCPeerConnection({
            iceServers: [{
                url: 'stun:23.21.150.121'
            }]
        });

        //当申请时
        //当 RTCIceCandidate 被添加到目标 RTCPeerConnection上时将会触发icecandidate 事件
        pc.onicecandidate = function (event) {
            if (!event || !event.candidate) return

            //发送申请给远程连接点
            ws.wsSend({
                'action' : 'candidate',
                'to': otherUserId,
                'candidate' :event.candidate
            });
        }

        //信号状态改变时
        pc.onsignalingstatechange = function(event) {

            console.log(pc.signalingState)

            /*
                have-local-offer

                stable         代表协商完成，   可以进行下一个协商。
                        其他状态代表协商中 ，  不可以进行下一个协商。

                closed
            */

            if (pc.signalingState === "have-local-pranswer") {
                // setLocalDescription() has been called with an answer
            }
        };




        pc.users ={};

        window.pc = pc;



        //设置编解码能力  需要进一步调用
        /*
        const transceiver = pc.getTransceivers().find(t => t.sender && t.sender.track === localStream.getVideoTracks()[0]);
        transceiver.setCodecPreferences([
            {
                "clockRate":90000,
                "mimeType":"video/H264",
                "sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=42e01f"
            },
            {
                "clockRate":90000,
                "mimeType":"video/VP8"
            },
            {
                "clockRate":90000,
                "mimeType":"video/rtx"
            },{"clockRate":90000,"mimeType":"video/VP9","sdpFmtpLine":"profile-id=0"},
            {"clockRate":90000,"mimeType":"video/VP9","sdpFmtpLine":"profile-id=2"},
            {"clockRate":90000,"mimeType":"video/H264","sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=42001f"},
            {"clockRate":90000,"mimeType":"video/H264","sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=0;profile-level-id=42001f"},
            {"clockRate":90000,"mimeType":"video/H264","sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=42e01f"},
            {"clockRate":90000,"mimeType":"video/H264","sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=0;profile-level-id=42e01f"},
            {"clockRate":90000,"mimeType":"video/H264","sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=4d0032"},
            {"clockRate":90000,"mimeType":"video/H264","sdpFmtpLine":"level-asymmetry-allowed=1;packetization-mode=1;profile-level-id=640032"},
            {"clockRate":90000,"mimeType":"video/red"},
            {"clockRate":90000,"mimeType":"video/ulpfec"}
        ]);
        */


        /*
        注意点
            远端的pc.onaddstream的调用条件
                1. 本地端添加视频      pc.addStream(localStream)
                2. 本地端与远端协商
                     本地  发送offer给    远端
                     远端  发送answer给   本地
        */
        pc.onaddstream = function (event) {

            debugger


            window.remoteStream = event.stream;
            controllStream()

            showStream(remoteStream, event.target.users.he)
        };


        pc.onremovestream = function(ev) {
            debugger


            document.querySelectorAll('video').forEach((ele,i)=>{

                if(ele.parentElement.getAttribute('streamId') == ev.stream.id){


                    deleteStream({
                        userId: ele.parentElement.getAttribute('id')
                    })
                }
            })



        };

        pc.ontrack = function(event) {
            //debugger
            var audio = document.createElement('audio');
            audio.srcObject = event.streams[0];
            document.body.appendChild(audio)


            // document.getElementById("received_video").srcObject = event.streams[0];
            // document.getElementById("hangup-button").disabled = false;
        };







        pc.onconnectionstatechange = function (event) {


            switch(pc.connectionState) {
                case "connected":
                    // The connection has become fully connected
                    break;
                case "disconnected":
                case "failed":
                    // One or more transports has terminated unexpectedly or in an error
                    break;
                case "closed":
                    // The connection has been closed
                    break;
            }
        }






        pc.ondatachannel = function (event) {

            event.channel.onopen = function() {
                console.log('Data channel is open and ready to be used.');
            };

        }

        var channel = pc.createDataChannel("Mydata");
        console.log(channel)
        channel.onopen = function(event) {

            channel.send('sending a message');
        }
        channel.onmessage = function(event) {
            debugger
            console.log(event.data);
        }

        channel.onbufferedamountlow = ()=>{

        }
        channel.onerror = ()=>{
            debugger
        }


        //iceServer 连接状态改变时
        pc.oniceconnectionstatechange = function(event) {
            if (pc.iceConnectionState === "failed" ||
                pc.iceConnectionState === "disconnected" ||
                pc.iceConnectionState === "closed") {
                // Handle the failure
            }

            /*

                # new — WebRTC引擎正在等待通过调用RTCPeerConnection.addIceCandidate（）来接收远程候选对象。

                # checking – WebRTC引擎已收到远程候选，并正在比较本地和远程候选以尝试找到合适的匹配。

                # connected – 已经确定了一对适当匹配的候选并建立了连接。根据Trickle ICE协议，候选项仍可以继续共享。

                # completed – WebRTC引擎已经完成了收集候选项，已经检查了所有候选对，并找到了所有组件的连接。

                # failed – WebRTC引擎查找了所有候选对，但是未能找到合适的匹配。

                # disconnected –RTCPeerConnection中至少有一个轨道已断开连接。这可能会间歇性地触发并在不可靠的网络上自行解决。在这种情况下，连接状态可能会改回“已连接”。

                # closed –RTCPeerConnection实例已关闭，并且不再处理请求。

                通常，如果状态变为failed，可能需要循环两端机器上的每个通过的ICE候选包，以辨别失败发生的原因；例如，如果一方只提供host和srflx数据包，而另一方提供host和relay数据包，但双方都在不同的网络上。
           */

        };

        // ??????
        pc.onicegatheringstatechange = function() {

            console.log('pc.iceGatheringState:    '+ pc.iceGatheringState)


        }

        //身份认证
        pc.onidentityresult = function(ev) {
            alert("onidentityresult event detected!");
        };



        //需要协商时
        pc.onnegotiationneeded = function() {

        }




        //获取发送过的视频流
        pc.getSenders()

        //身份认证
        pc.peerIdentity


        //会话描述（sessionDescription） （描述了本地端或者远程端 连接的状态）

        //localDescription
        var sd = pc.pendingLocalDescription;
        var sd = pc.pendingRemoteDescription;
        var sd = pc.remoteDescription;



        //sctp  (StreamControlTransmissionProtocol)
        pc.sctp









        /*

        //关闭连接
        pc.close()
        */


        window.pc =pc;
        return pc;
    }


    //主动发送本地流
    function initiativeSendLocalStream(){

        if(!pcMaps[otherUserId]){
            pc = pcMaps[otherUserId] = createPeerConnection()



        }else{
            pc = pcMaps[otherUserId]
        }





        if(0){
            //测试 start
            otherUserId = 'screen'


            pc.removeStream(localStream)
            pc.addStream(localStream)
            pc.createOffer(function (offer/*寻找远端匹配机器（peer）的请求（带有特定的配置信息）*/) {

                console.log('lllllll')
                console.log(offer.sdp)
                pc.setLocalDescription(offer)

                ws.wsSend({
                    'action' : 'create',
                    'to': otherUserId,
                    'offer':offer
                });
            }, function() {}, {
                optional: [],
                mandatory: {
                    OfferToReceiveAudio: false,
                    OfferToReceiveVideo: true
                }
            })
            //测试 end
        }




        //添加视频流, 触发 onnegotiationneeded,  只有协商完成后，远端才能使用 这个流
        pc.addStream(localStream)




        pc.createOffer(function (offer/*寻找远端匹配机器（peer）的请求（带有特定的配置信息）*/) {
            console.log('lllllll')
            console.log(offer.sdp)

            pc.setLocalDescription(offer)


            pc.users.me=userId;
            pc.users.he=otherUserId;

            ws.wsSend({
                'action' : 'create',
                'to': otherUserId,
                'offer':offer
            });
        }, function() {}, {
            optional: [],
            mandatory: {
                OfferToReceiveAudio: false,
                OfferToReceiveVideo: true
            }
        })
    }






    //展示视频流
    function showStream(stream,streamUserId){










        var wrap = document.createElement('div');

        var video = document.createElement('video');

        video.srcObject = stream;

        //通过此方法，获取video标签的视频流
       // var captrueStream = video.captureStream()

        video.oncanplay = (ev)=>{

        }
        video.oncanplaythrough = ()=>{

        }

        video.onanimationstart = ()=>{

        }
        video.ongotpointercapture =()=>{

        }

        video.onresize = () => {

             /*




             */

        };

        video.onloadedmetadata = function(e) {
            // 解决报错问题： Uncaught (in promise) DOMException: play() failed because the user didn't interact
            // 参考  https://blog.csdn.net/yhblog/article/details/83183687
            video.muted = true;

            if(type == 'meeting'){
                if(streamUserId == userId){
                    wrap.className='video '+ 'me ' + 'meeting'

                }else{
                    wrap.className='video '+ 'other ' + 'meeting'

                }
            }else{

                if(streamUserId == userId){
                    wrap.className='video '+ 'me ' + 'screenShare'

                }else{
                    wrap.className='video '+ 'other ' + 'screenShare'

                }
            }
            wrap.id = streamUserId

            wrap.setAttribute('streamId',stream.id)



            document.body.appendChild(wrap)
            video.play();
        };
        wrap.appendChild(video)


        var toggleAudio = document.createElement('div');
        toggleAudio.innerHTML = '关闭声音';
        toggleAudio.onclick=()=>{
            if(toggleAudio.innerHTML == '打开声音'){
                stream.getAudioTracks().forEach((audioTrack)=>{
                    audioTrack.enabled = true;
                })
                toggleAudio.innerHTML = '关闭声音';
            }else{
                stream.getAudioTracks().forEach((audioTrack)=>{
                    audioTrack.enabled = false;
                })
                toggleAudio.innerHTML = '打开声音';
            }


        }
        wrap.appendChild(toggleAudio)


        var toggleVideo = document.createElement('div');
        toggleVideo.innerHTML = '遮住视频';
        toggleVideo.onclick=()=>{
            if(toggleVideo.innerHTML == '显示视频'){
                stream.getVideoTracks().forEach((videoTrack)=>{
                    videoTrack.enabled = true;


                    /*
                        const track = mediaStream.getVideoTracks()[0]

                        //获取约束
                        track.getConstraints()
                     */


                })
                toggleVideo.innerHTML = '遮住视频';
            }else{
                stream.getVideoTracks().forEach((videoTrack)=>{
                    videoTrack.enabled = false;
                })
                toggleVideo.innerHTML = '显示视频';
            }


        }
        wrap.appendChild(toggleVideo)



        //删除视频
        var updateVideo = document.createElement('div');
        updateVideo.innerHTML = '更新视频';
        updateVideo.onclick=()=>{
            openStream()
        }
        wrap.appendChild(updateVideo)









        if(0){



            //声音控制相关  start
            //问题： 还是有啸叫   回声消除，降噪



            //可以创建节点，处理音频、解码

            var audioCtx = new AudioContext({
                sampleRate:16000  //设置采样率
            });

            window.audioCtx =audioCtx;

            //创建一个 来源节点（用来关联音频流）.
            var source = audioCtx.createMediaStreamSource(stream);


            //构造参数依次为缓冲区大小，输入通道数，输出通道数
            var scriptNode = audioCtx.createScriptProcessor(
                4096/*bufferSize*/,
                1/*numberOfInputChannels*/,
                1/*numberOfOutputChannels*/
            );

            //此事件可以对声音进行处理
            scriptNode.onaudioprocess = function (audioProcessingEvent) {

                var inputBuffer = audioProcessingEvent.inputBuffer;

                // The output buffer contains the samples that will be modified and played
                var outputBuffer = audioProcessingEvent.outputBuffer;




                // Loop through the output channels (in this case there is only one)
                for (var channel = 0; channel < outputBuffer.numberOfChannels; channel++) {
                    var inputData = inputBuffer.getChannelData(channel);
                    var outputData = outputBuffer.getChannelData(channel);

                    // Loop through the 4096 samples
                    for (var sample = 0; sample < inputBuffer.length; sample++) {
                        // make output equal to the same as the input
                        outputData[sample] = inputData[sample];

                        // add noise to each output sample
                        //outputData[sample] += ((Math.random() * 2) - 1) * 0.2;
                    }
                }


            }




            // 立体声
            // debugger
            // var channels = 1;
            // // 创建一个 采样率与音频环境(AudioContext)相同的 时长2秒的 音频片段。
            // var frameCount = audioCtx.sampleRate * 2.0;
            //
            // var myArrayBuffer = audioCtx.createBuffer(
            //     channels  /*Number numOfChannels*/,
            //     frameCount  /* frameCount */,
            //     audioCtx.sampleRate /*Number sampleRate*/
            // )
            //
            //
            //
            //
            // var buffer = new Float32Array(frameCount);
            //
            //
            //
            //
            // for (var channel = 0; channel < channels; channel++) {
            //     // 这允许我们读取实际音频片段(AudioBuffer)中包含的数据
            //     var nowBuffering = myArrayBuffer.getChannelData(channel);
            //     for (var i = 0; i < frameCount; i++) {
            //         // Math.random() is in [0; 1.0]
            //         // audio needs to be in [-1.0; 1.0]
            //         nowBuffering[i] = Math.random() * 2 - 1;
            //
            //         //nowBuffering[i] = buffer[i * (channel + 1)];
            //     }
            // }
            //
            // // 获取一个 音频片段源节点(AudioBufferSourceNode)。
            // // 当我们想播放音频片段时，我们会用到这个源节点。
            // var source = audioCtx.createBufferSource();
            // // 把刚才生成的片段加入到 音频片段源节点(AudioBufferSourceNode)。
            // source.buffer = myArrayBuffer;
            // // 把 音频片段源节点(AudioBufferSourceNode) 连接到
            // // 音频环境(AudioContext) 的终节点，这样我们就能听到声音了。
            // source.connect(audioCtx.destination);
            // // 开始播放声源
            // source.start();



















            //异步解码音频文件.
            //AudioContext.decodeAudioData

            //显示音频时间和频率的数据
            //audioCtx.createAnalyser()


            //从多个音频流信道结合成一个单一的音频流
            //audioCtx.createChannelMerger()

            //用于访问的音频流的单独的通道并分别对他们进行处理。
            // audioCtx.createChannelSplitter()

            //创建一个 双二阶滤波器
            var biquadFilter = audioCtx.createBiquadFilter();
            //滤波器类型：高通、低通、带通
            biquadFilter.type = "lowshelf";
            biquadFilter.frequency.value = 1;
            //音量控制
            biquadFilter.gain.value = 0;


            var myFrequencyArray = new Float32Array(5);
            myFrequencyArray[0] = 1000;
            myFrequencyArray[1] = 2000;
            myFrequencyArray[2] = 3000;
            myFrequencyArray[3] = 4000;
            myFrequencyArray[4] = 5000;
            var magResponseOutput = new Float32Array(5);
            var phaseResponseOutput = new Float32Array(5);

            biquadFilter.getFrequencyResponse(myFrequencyArray, magResponseOutput, phaseResponseOutput);





            //声音来源节点 连接 过滤器
            // source.connect(biquadFilter);
            // //过滤器      连接 声音输出节点
            // biquadFilter.connect(audioCtx.destination);



            source.connect(scriptNode);
            //过滤器      连接 声音输出节点
            scriptNode.connect(audioCtx.destination);

            //声音控制相关  end

            //window.a = true
        }










    }












    function parseUrl(url){
        var a = document.createElement('a');
        a.href = url;
        return {
            host: a.hostname,
            query: a.search,
            params: (function () {
                var ret = {},
                    seg = a.search.replace(/^\?/, '').split('&'),
                    len = seg.length,
                    i = 0,
                    s;
                for (; i < len; i++) {
                    if (!seg[i]) {
                        continue;
                    }
                    s = seg[i].split('=');
                    //ret[s[0]] = s[1];
                    ret[s[0]] = decodeURIComponent(s[1]);


                }
                return ret;
            })(),
            hash: a.hash.replace('#', '')
        };
    }










    navigator.mediaDevices.addEventListener('devicechange', event => {

        console.log(event)
    });


    /*

    MediaStream
        包含多个 MediaStreamTrack（声轨和视频轨）
            包含多个通道 （代表着媒体流的最小单元，比如一个音频信号对应着一个对应的扬声器）

    */
    var newStream = new MediaStream();













    // var msg = '["call2",["voip_invite",8,{"line":"9909719045","channelName":"274541","peer":"sipgw_mxj13671398016","extra":"{\\"destMediaUid\\":12447133}"}]]'
    //
    // websocket11.send(msg)


















</script>
</html>
